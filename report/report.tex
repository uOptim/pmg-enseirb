\documentclass{article}

\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{calc}
\usepackage{color}
\usepackage{float}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{tikz}

\geometry{margin=2cm}

\title{Projet système \\ Mise en place d'une bibliothèque de threads}
\author{Benoît Ruelle, Ludovic Hofer}

\begin{document}
\begin{center}
  \includegraphics [width=40mm]{ENSEIRB-MATMECA.jpg} 

\vspace{\stretch{1}}

\textsc{\Huge Simulation de particules sur GPU}\\[0.5cm]
\rule{0.4\textwidth}{1pt}

\vspace{\stretch{1}}

\begin{center}
  
  \begin{flushleft}
    \large
    \emph{Auteurs :}\\
    \begin{itemize}
    \item Benoît Ruelle
    \item Ludovic Hofer
    \end{itemize}
  \end{flushleft}
  
  
  \begin{flushright}
    \large
    \emph{Encadrant :}\\
    Raymond Namyst
  \end{flushright}
\end{center}

\vspace{\stretch{1}}

{\large \url{https://github.com/uOptim/pmg-enseirb/}}

\vspace{\stretch{1}}

{\large Deuxième année, filière informatique} 
~

{\large 14 avril 2013 - 6 juin 2013}\\

\end{center}
\thispagestyle{empty}
\pagebreak
\tableofcontents
\listoffigures
\newpage

\section{Introduction}
Le but de ce projet qui s'inscrit dans le cadre du cours de programmation
multicoeur et GPU est de concevoir une application de simulation de
particules en utilisant openCL. Une visualisation de la situation en temps
réel utilisant OpenGL nous a été fournie afin que nous puissions concentrer
nos efforts sur l'accélération des calculs en OpenCL.

\section{Développement et difficultés rencontrées}

\subsection{Division par zéro dans le cas des collisions}

\paragraph{}
Un des premiers problèmes auxquels nous avons du faire face se situait dans le
noyau \verb!collision!. En effet, lors de l'exécution de la configuration
\verb!bounce.conf!, si la collision était activée dès le début, le
comportement obtenu correspondait à nos attentes, mais si nous laissions les
atomes se traverser une fois avant d'activer la collision, les deux atomes
disparaissaient lors de la collision.

\paragraph{}
En nous penchant plus en détail sur ce problème, nous nous sommes aperçus que le
problème venait du fait qu'à plusieurs endroits de l'équation, on divisait un
élément par $1 + i.x$. Or, dans le cas évoqué ci-dessus, $i.x = -1$, on effectue
donc une division par zéro. Pour pallier ce problème, nous avons simplement
traité ce cas en inversant le vecteur de distance dans ce cas précis.

\subsection{Mise en place de la mesure}
\paragraph{}
Pour mesurer le temps d'exécution, nous avons essayé de mesurer celui-ci en
effectuant la différence entre le temps au début de la fonction idle et à la
fin de celle-ci, ceci afin de mesurer réellement le nombre d'image par seconde.
\paragraph{}
Cette première méthode nous a posé problème étant donné que nous avions un temps
minimal d'environ 16 millisecondes, ce quelque soit les modes et le nombre
d'atomes utilisés. Le temps de 16 millisecondes correspondant étrangement avec
60 images par seconde, nous avons eu un axe pour la recherche du problème, nous
avons rapidement pu déterminer deux sources possibles, la présence d'une
synchronisation verticale et l'utilisation du double buffer. Si nous ne sommes
pas parvenu à enlever la synchronisation verticale, l'utilisation du simple
buffer nous a permis de dépasser les 60 images par secondes et d'avoir ainsi des
mesures plus intéressantes.
\paragraph{}
Étant donné que le rendu graphique n'était que secondaire, nous avons finalement
dérivé sur des mesures de temps avant et après les
\verb!clEnqueueNDRangeKernel!. Afin que la tâche soit réellement mesurée, il
a aussi été nécessaire que nous imposions un \verb!clFinish! avant de prendre
la mesure du temps.

\subsection{Collision v3}
Afin de d'optimiser l'exécution de la fonction collision, nous avons cherché à
rassembler les threads en workgroups afin qu'ils partagent de la mémoire.

\subsubsection{Répartition des workgroups/threads}
\begin{figure}[p]
  \caption{Répartition des workgroups pour collision\_v3}
  \label{collision-v3}
  \resizebox{\columnwidth}{!}{
    \begin{tikzpicture}[font=\Large]
      \pgfmathsetmacro{\nbthreads}{32}
      \pgfmathsetmacro{\wgsize}{16}
      %collision rectangles
      \foreach \x in {2, ..., \nbthreads} {
        \foreach \y in {2, ..., \x} {
          \draw [ultra thick] (\y, \nbthreads - \x) rectangle (\y + 1, \nbthreads - \x - 1);
        }
      }
      % threads numbers
      \foreach \x in {1, ..., \nbthreads} {
        \node at (1.5 , \nbthreads - 0.5 - \x) {\x};
      }
      % work rectangles
      \draw [red] (2, \nbthreads - 1) rectangle ( 18, \nbthreads - 1 - \wgsize);
      \draw [red] (2, \nbthreads - 17) rectangle ( 18, \nbthreads - 17 - \wgsize);
      \draw [red] (18, \nbthreads - 17) rectangle ( 34, \nbthreads - 17 - \wgsize);
      \node [red, scale=10] at (10,\nbthreads -9) {0};
      \node [red, scale=10] at (10,\nbthreads -25) {1};
      \node [red, scale=10] at (26,\nbthreads -25) {2};
    \end{tikzpicture}
  }
\end{figure}
\paragraph{}
Afin de simplifier la notation, on notera par la suite la taille d'un
workgroup par $WG\_SIZE$.
\paragraph{}
On peut voir dans le schéma suivant (figure~\ref{collision-v3})
que chaque workgroup n'aura besoin de la mémoire
que de $2 * WG\_SIZE$ atomes pour couvrir $WG\_SIZE^2$ collisions de plus
un avantage est que la mémoire est à chaque fois contiguë, il n'y a donc en
réalité que deux accès mémoire nécessaires. À l'intérieur de chaque workgroup,
un thread calculera uniquement les collisions sur sa ligne. Chaque thread
calculera donc $WG\_SIZE$ collisions au maximum.
\subsubsection{Calcul de la position en fonction du group id}
Afin de pouvoir respecter la répartition en workgroups présentée ci-dessus, il
est nécessaire de pouvoir retrouver la position d'un groupe à partir de l'id du
groupe. Afin d'obtenir la ligne, il est nécessaire de trouver le premier $l$ tel
que $l* (l+1) < g$ avec $g$ le numéro du groupe.
\paragraph{}
Si l'on définit la position du groupe par $(x,y)$ avec $x$ la ligne et $y$ la
colonne on peut effectuer le raisonnement suivant :
$$\frac{(x-1) * x}{2} < g \leq \frac{x * (x+1)}{2}$$
$$(x-1) * x < 2g \leq x * (x+1)$$
$$(x-1)^2 < 2d \leq (x+1)^2$$
$$x-1 < \sqrt{2d} \leq (x+1)$$
\paragraph{}
L'utilisation de cette racine permet donc de rapidement réduire les possibilités
à deux candidats pour le numéro de la ligne, il est donc ensuite facile de
déterminer le numéro de la ligne

\subsubsection{Complexité}
Cette démarche nous assure non seulement un nombre de threads élevé, mais aussi
de la mémoire partagée à l'intérieur du workgroup, de façon à réduire le nombre
d'accès mémoire par collision. Cependant, chaque thread doit commencer par
effectuer la racine carrée d'un nombre de l'ordre du carré du nombre d'atomes,
cette opération ne s'effectuant pas forcément en temps constante, elle
représente une partie importante du temps d'exécution si chaque thread contrôle
peu de collisions.
\subsubsection{En faisant varier la taille des workgroups}
Le nombre de threads dépendant de $(n/ WG\_SIZE)^2$ dans cette implémentation,
il semble naturel qu'avec un $WG\_SIZE$ trop petit, le nombre de threads sera
trop élevé par rapport à la longueur de la tâche. Il nous a donc semblé
intéressant d'observer l'évolution du temps d'exécution en changeant la valeur
de $WG\_SIZE$.
\paragraph{}
À cette fin, nous avons calqué $WG\_SIZE$ sur l'alignement donné en macro et
nous avons ensuite modifié cette valeur.

\subsection{Lennard Jones v2}
Dans le cas du calcul des forces avec la méthode de Lennard-Jones, il n'est pas
profitable d'effectuer une seule fois les calculs de force en profitant de la
symétrie de la situation. En effet, cela nous obligerait à mettre en place des
mécanismes permettant d'éviter une écriture concurrentielle, le coût engendré
par ces attentes serait plus élevé que le gain qu'il apporterait.
En revanche, il est parfaitement possible d'utiliser le partage de la mémoire
par des groupes de thread, en opérant par tranche. Et en imposant une barrière
locale à chaque fin de tranche et après chaque chargement de tranche.
Chaque thread continue donc à calculer les forces pour une seule ligne, mais le
partage de la mémoire permet d'économiser de parallèliser le chargement et donc
d'économiser du temps. \ref{lennard-jones-v2}
\begin{figure}[p]
  \caption{Organisation des workgroups pour Lennard-Jones v2}
  \label{lennard-jones-v2}
  \resizebox{\columnwidth}{!}{
    \begin{tikzpicture}[font=\Large]
      \pgfmathsetmacro{\nbthreads}{64}
      \pgfmathsetmacro{\wgsize}{16}
      \pgfmathsetmacro{\nbgroups}{\nbthreads / \wgsize}
      %force rectangles
      \foreach \x in {1, ..., \nbthreads} {
        \foreach \y in {1, ..., \nbthreads} {
          \draw [ultra thick] (\y, \nbthreads - \x) rectangle (\y + 1, \nbthreads - \x - 1);
        }
      }
      % threads numbers
      \foreach \x in {1, ..., \nbthreads} {
        \node at (0.5 , \nbthreads - 0.5 - \x) {\x};
      }
      % work rectangles
      \foreach \x in {1, 17, ..., \nbthreads} {
        \draw [red] (\nbthreads + 1, \x - 2) rectangle ( 1, \x + 14);
      }
      % barriers
      \foreach \y in {0,16, ...,\nbthreads} {
        \draw [violet, line width=6] (\y + 1, -1) -- (\y + 1, \nbthreads -1);
        \node [violet, scale=5] at (\y + 0.5, \nbthreads) {barrier};
        % text to add
      }
      % Workgroup text
      \foreach \x [evaluate=\x as \currentIndex using {int(\nbgroups - \x)}] in {1, ..., \nbgroups} {
        \foreach \y in {1, ..., \nbgroups} {
          \node [red, scale=10] at (\y * \wgsize - 7, \x * \wgsize - 8) {\currentIndex};
        }
      }
    \end{tikzpicture}
  }
\end{figure}
\paragraph{}
Comme pour la troisième version des collisions, nous avons décidé ici aussi de
laisser la possibilité de changer la taille des workgroups, celle-ci étant
toujours égale à l'alignement.

\subsection{Génération de fichiers pour les atomes}
Afin de pouvoir tester notre programme sur un très grand nombre d'atomes, nous
avons écrit un script python permettant de générer un fichier contenant un
nombre d'atomes spécifié à l'utilisation. Les positions et vitesse de ceux-ci
sont générés aléatoirement dans l'espace défini. Nous avons volontairement
choisi de fonctionner dans un très grand espace, car si des atomes commencent
trop proches l'un de l'autre, la force de répulsion qu'ils subissent les fait
totalement sortir de la zone dans laquelle ils ont commencé et ils ne sont
ensuite plus pris en compte pour le calcul, ce qui fausse le temps d'exécution.

\subsection{Script de mesure de performances}
Afin de simplifier la mesure des performances, nous avons choisi d'utiliser des
scripts bash pour lancer des séries de simulation. L'un sert à lancer une seule
simulation en capturant les temps d'exécution de chaque pas et en calculant leur
moyenne. L'autre permet de lancer le même test avec différents nombre d'atomes
afin de pouvoir facilement tracer un graphique ensuite.
\paragraph{}
Afin que l'exécution du programme commence directement avec certaines options,
nous avons modifié le code source du main en ajoutant d'autres options avec une
démarche semblable à celle utilisée pour l'option \verb!--full_speed!.

\section{Performances mesurées}

\subsection{Cas de la fonction collision}
\begin{figure}[p]
	\caption{Collisions : Comparaison des versions}
	\label{collisions-versions}
	\includegraphics[width=\textwidth]{figures/collision_versions.png}
\end{figure}
\begin{figure}[p]
	\caption{Collisions V3 : Comparaison des tailles de workgroup}
	\label{collisions-slices}
	\includegraphics[width=\textwidth]{figures/collision_slices.png}
\end{figure}

Les performances sont comparées en figure \ref{collisions-versions}. La différence entre la première et la seconde version est très faible. Bien
que dans la seconde version le travail donné à chaque thread est plus
équitable, leur nombre est deux fois plus faible que dans la première version.
En revanche, le gain du préchargement réalisé dans la troisième version est
net et d'un facteur dix.
\paragraph{}
Nous avons aussi observé une grande différence de performance en
changeant la taille de workgroup utilisée pour la troisième version
de l'implémentation (voir figure~\ref{collisions-slices}). On peut par
ailleurs observer une augmentation par palier lorsque la taille du groupe est
de 256.

\subsection{Cas du calcul des forces de Lennard-Jones}

Les performances des deux versions de Lennard-Jones sont comparées en figure
\ref{lennard-versions}. La différence entre les deux versions est très nette,
le facteur de temps étant de plus de 10 lorsque l'on approche des 2500 atomes.
Pour cette comparaison, il est important de noter que nous avons utilisé la
taille de workgroup par défaut, c'est à dire 16.
\begin{figure}[p]
	\caption{Lennard-Jones : Comparaison des versions}
	\label{lennard-versions}
	\includegraphics[width=\textwidth]{figures/lennard_versions.png}
\end{figure}
\paragraph{}
Dans la figure \ref{lennard-slices}, nous avons uniquement cherché à évaluer les
performances de la seconde version de Lennard-Jones en fonction de la taille de
workgroup utilisée. Le graphique obtenu permet de visualiser principalement deux
informations intéressantes.
\paragraph{Gain de performances~:}
La première information visible est qu'il y a un gain significatif de
performance en augmentant la taille du workgroup, cela peut être facilement
compris par le fait que l'on ne dépasse pas la mémoire disponible par workgroup
tout en diminuant fortement le nombre de barrières. Cela peut aussi être du au
fait que le nombre de workgroups demandés ne dépasse le nombre de workgroups
possibles en simultané.
\paragraph{Irrégularité des courbes obtenues~:}
Le meilleur exemple de cet aspect réside dans les augmentations soudaines du
temps d'exécution pour la version avec des workgroups de 16. En effet, on
voit ces augmentations soudaines aux environs de 1000, 2000, 3000 et 4000
\footnote{Notre pas de mesure étant de 100, il paraît raisonnable de supposer
que ces pics se situent en réalité à 1024, 2048, 3072 et 4096.}. Une explication
possible est que les cartes graphiques utilisées disposent de 64 workgroups
différents. En effet, dans ce cas, on obtient $64 * 16 = 1028$. Ce raisonnement
mène au tableau suivant :
\footnote{Ce tableau affiche le nombre minimal de workgroups qui devront se
partager une même unité de workgroup pour un certain nombre donné, par exemple,
lorsque 65 workgroups sont demandés pour 64 places, il faudra qu'au moins une
des places soit occupées par 2 workgroups}
$$
  \begin{tabular}{rcl||c|c|c}
    &nb\_atoms&  & WG\_SIZE = 16 & WG\_SIZE = 64 & WG\_SIZE = 256\\
    \hline \hline
       1&-&1028  &       1       &       1       &       1       \\
    1029&-&2056  &       2       &       1       &       1       \\
    2057&-&3072  &       3       &       1       &       1       \\
    3073&-&4096  &       4       &       1       &       1       \\
    4097&-&...   &       5       &       2       &       1
  \end{tabular}
$$
Le résultat obtenu est donc cohérent au vu de l'augmentation soudaine entre
4000 et 4100 pour la version avec des workgroups de taille 64. En revanche,
cette explication ne permet pas de comprendre l'évolution du temps d'exécution
pour une taille de workgroup de 256, celle-ci reste pour nous assez mystérieuse.

\begin{figure}[p]
	\caption{Lennard-Jones V2 : Comparaison de la taille des workgroups}
	\label{lennard-slices}
	\includegraphics[width=\textwidth]{figures/lennard_slices.png}
\end{figure}

\section{Perspectives}
Bien que nous ayons pu essayer différentes implémentations des différents
noyaux et que nous ayons pu estimer leur performances en fonction de la
taille de workgroup choisie, il serait intéressant de comparer notre
implémentation de collision qui est un peu particulière avec une version ou
chaque thread parcourt une ligne entière.
\paragraph{}
Un autre point intéressant pourrait être de valider la théorie que nous avons
émise à propos des changement brutaux de temps d'exécution
\footnote{cf figure \ref{lennard-slices}} et de chercher à expliquer ceux
que nous ne comprenons pas encore.

\section{Conclusion}
Au cours de ce projet, nous avons pu appréhender les difficultés propres à la
programmation sur GPU. En particulier, nous avons pu apprendre l'importance
du partage de la mémoire et l'impact du choix des workgroups dans ce type de
programmation. De plus nous avons pu expérimenter l'importance du choix de la
taille des workgroups. S'il est mieux de choisir classiquement une taille de
16 afin d'assurer des bons résultats avec tout type de carte graphique, il
nous paraît à présent assez clair que si l'on sait sur quel GPU notre code
est sensé tourner, il est intéressant d'effectuer quelques tests afin de
déterminer la taille de workgroup idéale pour un problème donné.

\end{document}
